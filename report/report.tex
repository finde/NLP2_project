\documentclass[11pt,DIV=11]{scrartcl}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{setspace}
%\onehalfspacing
\usepackage{parskip}
\usepackage{enumerate}
\usepackage{graphicx}
\renewcommand{\bf}[1]{\textbf{#1}}

\title{\small Natural Language Processing 2 - Project 2 Report \LARGE \\ \sc{\bf{Source Sentence Reordering for Better Translation}} }
\author{
    Xumara, Finde\\
    \texttt{finde.findexumara@student.uva.nl}
    \and
    Verdegaal, Jacob\\
    \texttt{jacob.verdegaal@student.uva.nl}
}

\begin{document}

\maketitle

\section{Introduction}
% why reordering, previous works
% problem of reordering
% author solution -> ITG
% ITG explanation -> from Wu 1997

In this project you will experiment with learning a model of preordering, that is, a model that tries to predict a permutation of the input sentence which is close to target language word-order. In other words, you would like to preprocess input sentences so that they are as suitable as possible to monotone (or near monotone) translation.9 Reasoning about arbitrary permutations is an NP-complete problem, thus you will focus on binarizable permutations (Wu, 1997).
You will reimplement the model proposed by Tromble and Eisner (2009). The authors of this model propose to learn a pairwise preference matrix whose entries are a linear combination of rich contextual features associated with each possible pair of input words in a sentence. 

All binary bracketings of a sequence (and in fact, all binarizable permutations) can be efficiently packed in a CKY chart.

The authors introduce a recursive formula similar to the Inside algorithm which decomposes the benefit of swapping any pair of adjacent subsequences of the input in terms of the entries of the preference matrix. 

At training, “gold-standard” permutations are extracted from an automatically word-aligned parallel corpus (they make your “labels”). The parameters of the linear model are estimated using an average perceptron algorithm. For each training instance, a perceptron update is the result of comparing the feature representation of that instance’s gold-standard permutation with the feature representation of the best binarizable permutation under the current linear model.
Implementing and testing an extension to this model will be worth an extra point. There are several opportunities for improvements depending on where your interests lie. A few suggestions are:
1. replace optimisation by sampling and introduce a permutation distance metric in decoding (e.g. Kendall’s tau);
2. consider a larger space of permutations (i.e. including permutations that are not binarizable – note that the authors propose to iterate their algorithm in order to achieve that
Finally, you will evaluate your model in terms of the permutations it produces (the authors compare predictions to gold-standard using BLEU). Again, you can earn an extra point if you evaluate the performance of a phrase-based MT system (e.g. Moses) using preordered text as produced by your model (and variants).

This project is an opportunity to get your hands dirty with important algorithms (e.g. CKY, Inside, Viterbi, perceptron) and, depending on how far you choose to go with the evaluation, it may become an opportunity to experiment with a complete MT pipeline.10


Earlier papers showed that reordering of the words in the source sentence improves Statistical Machine Translation (SMT) results. 
Unfortunately, the number of possible reordering is NP-complete problem, or known as the Linear reordering Problem (LOP).

Tromble et. al. \cite{Tromble2009} propose to search only the space that is generated by an viterbi ITG parse of the source sentence.

First the source data was tagged by the Stanford standard POS tagger to obtain extra information to build features on.
We also created an ITG for it and extracted the viterbi parses for each sentence.
From the tagged data features (as described in Tromble) are listed up to 200000 for 1000 sentences for $t_b$ we used max length of 4.
Then features vectors the sentences are created and fed to a perceptron.

\begin{enumerate}
  \item features listed
  \item feature vectors build
  \item perceptron trained
  \item sentences permutated according to 1 swap of sub tree in viterbi parse
  \item new vectors build for new (permuted sentences)
  \item repeat from step 3 till measure gives low error
\end {enumerate}

\section{Data preprocessing}
\label{data_preprocess}

\section{Experiments}
\label{experiments}

\section{Results}
\label{results}

\section{Conclusion}
\label{conclusion}


\pagebreak
\bibliographystyle{apalike}
{\small
	\bibliography{bibliography}
}

\end{document}
