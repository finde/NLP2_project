\documentclass[11pt,DIV=11]{scrartcl}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{setspace}
%\onehalfspacing
\usepackage{parskip}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{booktabs}
\renewcommand{\bf}[1]{\textbf{#1}}

\title{\small Natural Language Processing 2 - Project 2 Report \LARGE \\ \sc{\bf{Source Sentence Reordering for Better Translation}} }
\author{
    Xumara, Finde\\
    \texttt{finde.findexumara@student.uva.nl}
    \and
    Verdegaal, Jacob\\
    \texttt{jacob.verdegaal@student.uva.nl}
}

\begin{document}

\maketitle

\section{Introduction}
Earlier papers showed that reordering of the words in the source sentence improves Statistical Machine Translation (SMT) results. 
The goal of this preordering task is so that they are as suitable as possible to monotone (or near monotone) translation.
Unfortunately, the number of possible reordering permutation is an NP-complete problem, or known as the Linear reordering Problem (LOP), thus we will focus on binarizable permutations \cite{Wu1997}.
\cite{Tromble2009} propose to learn a pairwise preference matrix whose entries are a linear combination of rich contextual features associated with each possible pair of input words in a sentence. 
In the other words, they proposed to search only the space that is generated by an viterbi ITG parse of the source sentence.

At preprocessing steps, we first do basic preprocessing task such as: tokenising, lowercasing, converting/removing unregistered UTF-8 Characters, completing the puctuation, discarding sentences longer than 10 words, and compound splitting.

At training, first the source data was tagged by the Stanford standard POS tagger to obtain extra information to build features on.
We then created an ITG for it and extracted the viterbi parses for each sentence.
All possible permutations are then derived from these viterbi parses.

To learn the parameters of the linear model, we created feature vectors from features template (as described in Tromble) and fed them to an average perceptron.
For each training instance, a perceptron update is the result of comparing the feature representation of that instanceâ€™s gold-standard permutation with the feature representation of the best binarizable permutation under the current linear model.
The "gold-standard" permutation are extracted from an automatically word-aligned parallel corpus (as the "labels"); and the best binarizable permutation are done by using local-step-search algorithm (also proposed by Tromble et. al.)

During training, we iterate the local search as described earlier. 
However, for decoding, as suggested in the paper, we only do a single step of local search, thus restricting reorderings to the ITG neighborhood of the original German. 
We created the development set from the same source as training set; and the test set from unseen dataset.

To complete the process, we train a translation model with moses \cite{Koehn2006}.
We used the original (without preordering) test set as a baseline, and compare it with the preordered test set using standard evaluation metric such as BLEU, METEOR and TER.

\section{Experiments and results}
\label{experiments-results}

We use German-English parallel corpora from europarl v7 dataset \cite{Koehn2005}.
The complete experiment setup is as follow:
\begin{itemize}
	\item Source Language: German
	\item Target Language: English
	\item Sentences length: 2 - 10
	\item Training iterations: 5
	\item Maximum extracted features: 20000
	\item Training set size: 125
	\item Development set size: 125
	\item Test set size: 125
\end{itemize}

From 125 sentences with max length of 10 and minimal 2 words, listed up to 117387 long feature matrix.
This number will grow exponentially if we increase the maximum sentences length.
We initially want to do the experiment with more variant, for example use more data (1000 sentence-pair), bidirectional source-target language, longer sentences length and also different training iteration.
However, the resources is very limited and we can not really use the multi-processors trick because the memory is also limited.

Table \ref{evaluation} shows the results of our experiment evaluated with multEval \cite{Clark2011}.
This result indicate that the preordered mode improves SMT results on all metric.

\begin{table}[h]
	\centering
	\begin{tabular}{@{}lccc@{}}
		\toprule
		& \multicolumn{1}{l}{BLEU(*)} & \multicolumn{1}{l}{METEOR} & \multicolumn{1}{l}{TER} \\ \midrule
		Baseline & 0.9 (16.32*) & 5.7 & 94.2 \\
		Preorder & \textbf{4.9} \textbf{(21.76*)} & \textbf{11.7} & \textbf{78.8} \\ \bottomrule
	\end{tabular}
	\caption{Experiment result with 125 sentences. BLEU* score are coming from Moses BLEU-evaluator.}
	\label{evaluation}
\end{table}

\section{Conclusion}
\label{conclusion}

The challenge of SMT is that the model can not really distinguish between good and bad translated sentences (in term of reordering) although SMT is quite accurate to translate sentence word by word.
In this report we showed that preordered model outperforms the baseline models on all metrics.
This experiment result shows that by reordering of the words in the source sentence so that they are suitable for monotone translation can help to improve SMT.

We did understand that our analysis is a bit too premature, because we have not yet experiment it with more variants (such as longer sentences, or other language).
Until time we submitted this report, we still run the experiment with more data out-of-personal-curiosity.
If there anyone want to know the result, please contact us.

\bibliographystyle{apalike}
{\small
	\bibliography{bibliography}
}

\end{document}
