\documentclass[11pt,twocolumn,DIV=11]{scrartcl}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{setspace}
%\onehalfspacing
\usepackage{parskip}
\usepackage{enumerate}
\usepackage{graphicx}
\renewcommand{\bf}[1]{\textbf{#1}}
\usepackage{natbib}
\bibliographystyle{apalike}

\title{\small Natural Language Processing 2 - Project 2 Report \LARGE \\ \sc{\bf{Source Sentence Reordering for Better Translation}} }
\author{
    Xumeira, Finde\\
    \texttt{@}
    \and
    Verdegaal, Jacob\\
    \texttt{jacob.verdegaal@student.uva.nl}
}

\begin{document}

\maketitle


\section{Introduction}

Earlier papers showed that reordering of the words in the source sentence improves SMT results. The number of possible reorderings is $n!$ though, and is known as the Linear reordering Probelm (LOP). Tromble et. al. propose to search only the space that is generated by an viterbi ITG parse of the source sentence.

First the source data was tagged by the Stanford standard POS tagger to obtain extra information to build features on.
We also created an ITG for it and extracted the viterbi parses for each sentence.
From the tagged data features (as described in Tromble) are listed up to 200000 for 1000 sentences for $t_b$ we used max length of 4.
Then features vectors the sentences are created and fed to a perceptron.

\begin{enumerate}
  \item fetaures listed
  \item feature vectors build
  \item perceptron trained
  \item sentences permutated according to 1 swap of sub tree in viterbi parse
  \item new vectors build for new (permuted sentences)
  \item repeat from step 3 till measure gives low error
\end {enumerate}



\section{Experiments}
\label{experiments}

\section{Results}
\label{results}

\section{Conclusion}
\label{conclusion}


\bibliography{bibliography.bib}
\end{document}
